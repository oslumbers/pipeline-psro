{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from itertools import repeat\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents.trainer import with_common_config, with_base_config\n",
    "from ray.rllib.models.catalog import MODEL_DEFAULTS\n",
    "from ray.rllib.utils import try_import_tf\n",
    "\n",
    "from mprl.scripts.poker_parallel_algos.utils.policy_config_keys import POKER_ARCH1_MODEL_CONFIG_KEY\n",
    "from mprl.rl.envs.opnspl.measure_exploitability_eval_callback import measure_exploitability_nonlstm\n",
    "from mprl.utility_services.cloud_storage import maybe_download_object, connect_storage_client, BUCKET_NAME\n",
    "from mprl.rl.sac.sac_policy import SACDiscreteTFPolicy\n",
    "from mprl.rl.common.stratego_preprocessor import STRATEGO_PREPROCESSOR, StrategoDictFlatteningPreprocessor\n",
    "from mprl.rl.envs.opnspl.poker_multiagent_env import POKER_ENV, KUHN_POKER, LEDUC_POKER, PARTIALLY_OBSERVABLE, PokerMultiAgentEnv\n",
    "from mprl.rl.common.sac_stratego_model import SAC_STRATEGO_MODEL\n",
    "from mprl.scripts.poker_parallel_algos.utils.metanash import get_fp_metanash_for_payoff_table\n",
    "from mprl.utility_services.payoff_table import PayoffTable\n",
    "from mprl.utils import datetime_str\n",
    "\n",
    "tf = try_import_tf()\n",
    "\n",
    "OBSERVATION_MODE = PARTIALLY_OBSERVABLE\n",
    "\n",
    "POLICY_CLASS = SACDiscreteTFPolicy\n",
    "POLICY_CLASS_NAME = SACDiscreteTFPolicy.__name__\n",
    "MODEL_CONFIG_KEY = POKER_ARCH1_MODEL_CONFIG_KEY\n",
    "\n",
    "MANAGER_SEVER_HOST = \"localhost\"\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_stats_for_single_payoff_table(payoff_table_key, experiment_name, poker_game_version, model_config_key):\n",
    "    POKER_ENV_CONFIG = {\n",
    "        'version': poker_game_version,\n",
    "    }\n",
    "\n",
    "    storage_client = connect_storage_client()\n",
    "\n",
    "    # If you use ray for more than just this single example fn, you'll need to move ray.init to the top of your main()\n",
    "    ray.init(address=os.getenv('RAY_HEAD_NODE'), ignore_reinit_error=True, local_mode=True)\n",
    "\n",
    "    model_config_file_path, _ = maybe_download_object(storage_client=storage_client,\n",
    "                                                      bucket_name=BUCKET_NAME,\n",
    "                                                      object_name=model_config_key,\n",
    "                                                      force_download=False)\n",
    "\n",
    "    with open(model_config_file_path, 'r') as config_file:\n",
    "        model_config = json.load(fp=config_file)\n",
    "\n",
    "    example_env = PokerMultiAgentEnv(env_config=POKER_ENV_CONFIG)\n",
    "\n",
    "    obs_space = example_env.observation_space\n",
    "    act_space = example_env.action_space\n",
    "\n",
    "    preprocessor = StrategoDictFlatteningPreprocessor(obs_space=obs_space)\n",
    "    graph = tf.Graph()\n",
    "    sess = tf.Session(config=tf.ConfigProto(device_count={'GPU': 0}), graph=graph)\n",
    "\n",
    "    def fetch_logits(policy):\n",
    "        return {\n",
    "            \"behaviour_logits\": policy.model.last_output(),\n",
    "        }\n",
    "\n",
    "    _policy_cls = POLICY_CLASS.with_updates(\n",
    "        extra_action_fetches_fn=fetch_logits\n",
    "    )\n",
    "\n",
    "    with graph.as_default():\n",
    "        with sess.as_default():\n",
    "            policy = _policy_cls(\n",
    "                obs_space=preprocessor.observation_space,\n",
    "                action_space=act_space,\n",
    "                config=with_common_config({\n",
    "                    'model': with_base_config(base_config=MODEL_DEFAULTS, extra_config=model_config),\n",
    "                    'env': POKER_ENV,\n",
    "                    'env_config': POKER_ENV_CONFIG,\n",
    "                    'custom_preprocessor': STRATEGO_PREPROCESSOR}))\n",
    "\n",
    "    def set_policy_weights(weights_key):\n",
    "        weights_file_path, _ = maybe_download_object(storage_client=storage_client,\n",
    "                                                 bucket_name=BUCKET_NAME,\n",
    "                                                 object_name=weights_key,\n",
    "                                                 force_download=False)\n",
    "        policy.load_model_weights(weights_file_path)\n",
    "\n",
    "    payoff_table_local_path, _ = maybe_download_object(storage_client=storage_client,\n",
    "                                                           bucket_name=BUCKET_NAME,\n",
    "                                                           object_name=payoff_table_key,\n",
    "                                                           force_download=False)\n",
    "\n",
    "    payoff_table = PayoffTable.from_dill_file(dill_file_path=payoff_table_local_path)\n",
    "    stats_out = {\n",
    "        'payoff_table_key': [],\n",
    "        'experiment_name': [],\n",
    "        'num_policies': [],\n",
    "        'exploitability': [],\n",
    "        'total_steps': [],\n",
    "        'total_episodes': [],\n",
    "    }\n",
    "\n",
    "    exploitability_per_generation = []\n",
    "    total_steps_per_generation = []\n",
    "    total_episodes_per_generation = []\n",
    "    num_policies_per_generation = []\n",
    "\n",
    "    for i, n_policies in enumerate(range(1,payoff_table.size() + 1)):\n",
    "        metanash_probs = get_fp_metanash_for_payoff_table(payoff_table=payoff_table,\n",
    "                                                                 fp_iters=40000,\n",
    "                                                                 accepted_opponent_policy_class_names=[POLICY_CLASS_NAME],\n",
    "                                                                 accepted_opponent_model_config_keys=[POKER_ENV_CONFIG],\n",
    "                                                                 add_payoff_matrix_noise_std_dev=0.000,\n",
    "                                                                 mix_with_uniform_dist_coeff=None,\n",
    "                                                                 only_first_n_policies=n_policies,\n",
    "                                                                 p_or_lower_rounds_to_zero=0.0)\n",
    "\n",
    "        policy_weights_keys = payoff_table.get_ordered_keys_in_payoff_matrix()\n",
    "\n",
    "        policy_dict = {key: prob for key, prob in zip(policy_weights_keys, metanash_probs)}\n",
    "\n",
    "        exploitability_this_gen = measure_exploitability_nonlstm(rllib_policy=policy,\n",
    "                                  poker_game_version=poker_game_version,\n",
    "                                  policy_mixture_dict=policy_dict,\n",
    "                                  set_policy_weights_fn=set_policy_weights)\n",
    "\n",
    "        print(f\"{n_policies} policies, {exploitability_this_gen} exploitability\")\n",
    "\n",
    "        policy_added_this_gen = payoff_table.get_policy_for_index(i)\n",
    "        latest_policy_tags = policy_added_this_gen.tags\n",
    "        steps_prefix = \"timesteps: \"\n",
    "        latest_policy_steps = int([tag for tag in latest_policy_tags if steps_prefix in tag][0][len(steps_prefix):])\n",
    "        episodes_prefix = \"episodes: \"\n",
    "        latest_policy_episodes = int([tag for tag in latest_policy_tags if episodes_prefix in tag][0][len(episodes_prefix):])\n",
    "\n",
    "        if i > 0:\n",
    "            total_steps_this_generation = latest_policy_steps + total_steps_per_generation[i-1]\n",
    "            total_episodes_this_generation = latest_policy_episodes + total_episodes_per_generation[i-1]\n",
    "        else:\n",
    "            total_steps_this_generation = latest_policy_steps\n",
    "            total_episodes_this_generation = latest_policy_episodes\n",
    "\n",
    "        exploitability_per_generation.append(exploitability_this_gen)\n",
    "        total_steps_per_generation.append(total_steps_this_generation)\n",
    "        total_episodes_per_generation.append(total_episodes_this_generation)\n",
    "        num_policies_per_generation.append(n_policies)\n",
    "\n",
    "        num_new_entries = len(exploitability_per_generation)\n",
    "        stats_out['payoff_table_key'] = stats_out['payoff_table_key'] + [payoff_table_key] * num_new_entries\n",
    "        stats_out['experiment_name'] = stats_out['experiment_name'] + [experiment_name] * num_new_entries\n",
    "        stats_out['num_policies'] = stats_out['num_policies'] + num_policies_per_generation\n",
    "        stats_out['exploitability'] = stats_out['exploitability'] + exploitability_per_generation\n",
    "        stats_out['total_steps'] = stats_out['total_steps'] + total_steps_per_generation\n",
    "        stats_out['total_episodes'] = stats_out['total_episodes'] + total_episodes_per_generation\n",
    "    return stats_out\n",
    "\n",
    "def get_exploitability_stats_over_time_for_payoff_table_all_same_poker_version(\n",
    "        payoff_table_keys, exp_names, poker_game_version, model_config_key):\n",
    "\n",
    "    with multiprocessing.Pool(processes=16) as pool:\n",
    "        results = pool.starmap(func=get_stats_for_single_payoff_table,\n",
    "                               iterable=zip(payoff_table_keys, exp_names, repeat(poker_game_version), repeat(model_config_key)))\n",
    "\n",
    "    combined_stats = {}\n",
    "    for result in results:\n",
    "        for key, val in result.items():\n",
    "            if key not in combined_stats:\n",
    "                combined_stats[key] = val\n",
    "            else:\n",
    "                combined_stats[key] = [*combined_stats[key], *val]\n",
    "\n",
    "    return pd.DataFrame(combined_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Graph the results of the original paper Leduc poker experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "google_cloud_leduc_experiment_payoff_tables_and_names = [\n",
    "    (\"leduc_poker_pipe_3_workers_poker_ps/leduc_pipeline_psro/pipe-1-3-3-leduc-poker_pid_430_09_03_49AM_Jun-01-2020/payoff_tables/latest.dill\",\"leduc_pipe_1\"),\n",
    "    (\"leduc_poker_pipe_3_workers_poker_ps/leduc_pipeline_psro/pipe-2-3-3-leduc-poker_pid_430_09_04_00AM_Jun-01-2020/payoff_tables/latest.dill\",\"leduc_pipe_2\"),\n",
    "    (\"leduc_poker_pipe_3_workers_poker_ps/leduc_pipeline_psro/pipe-3-3-3-leduc-poker_pid_431_09_04_01AM_Jun-01-2020/payoff_tables/latest.dill\",\"leduc_pipe_3\"),\n",
    "    (\"leduc_poker_rect_3_workers_poker_ps/leduc_psro_rectified/rect-1-3-3-leduc-poker_pid_429_09_04_14AM_Jun-01-2020/payoff_tables/latest.dill\",\"leduc_rect_1\"),\n",
    "    (\"leduc_poker_rect_3_workers_poker_ps/leduc_psro_rectified/rect-2-3-3-leduc-poker_pid_430_09_04_24AM_Jun-01-2020/payoff_tables/latest.dill\",\"leduc_rect_2\"),\n",
    "    (\"leduc_poker_rect_3_workers_poker_ps/leduc_psro_rectified/rect-3-3-3-leduc-poker_pid_430_09_04_45AM_Jun-01-2020/payoff_tables/latest.dill\",\"leduc_rect_3\"),\n",
    "    (\"leduc_poker_naive_3_workers_poker_ps/leduc_psro_naive/naive-1-3-3-leduc-poker_pid_430_09_03_04AM_Jun-01-2020/payoff_tables/latest.dill\",\"leduc_naive_1\"),\n",
    "    (\"leduc_poker_naive_3_workers_poker_ps/leduc_psro_naive/naive-2-3-3-leduc-poker_pid_430_09_03_16AM_Jun-01-2020/payoff_tables/latest.dill\",\"leduc_naive_2\"),\n",
    "    (\"leduc_poker_naive_3_workers_poker_ps/leduc_psro_naive/naive-3-3-3-leduc-poker_pid_430_09_03_34AM_Jun-01-2020/payoff_tables/latest.dill\",\"leduc_naive_3\")\n",
    "]\n",
    "gc_leduc_table_keys, gc_leduc_exp_names = zip(*google_cloud_leduc_experiment_payoff_tables_and_names)\n",
    "gc_leduc_perf_df = get_exploitability_stats_over_time_for_payoff_table_all_same_poker_version(\n",
    "    payoff_table_keys=gc_leduc_table_keys,\n",
    "    exp_names=gc_leduc_exp_names,\n",
    "    poker_game_version=\"leduc_poker\",\n",
    "    model_config_key=POKER_ARCH1_MODEL_CONFIG_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(gc_leduc_perf_df.drop_duplicates(), x=\"total_episodes\", y=\"exploitability\", title=f\"Exploitability over Episodes Leduc 3 workers\",\n",
    "        render_mode=\"svg\", color=\"experiment_name\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_leduc_perf_df.to_csv(\"gc_leduc_jun_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Example for graphing results of a Kuhn Poker Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kuhn_experiment_payoff_tables_and_names = [\n",
    "    (\"kuhn_poker_pipe_3_workers_poker_ps/kuhn_pipeline_psro/goku_pid_143271_02_37_42AM_Oct-12-2020/payoff_tables/latest.dill\",\"kuhn_pipe_1\"),\n",
    "]\n",
    "kuhn_table_keys, kuhn_exp_names = zip(*kuhn_experiment_payoff_tables_and_names)\n",
    "kuhn_perf_df = get_exploitability_stats_over_time_for_payoff_table_all_same_poker_version(\n",
    "    payoff_table_keys=kuhn_table_keys,\n",
    "    exp_names=kuhn_exp_names,\n",
    "    poker_game_version=\"kuhn_poker\",\n",
    "    model_config_key=POKER_ARCH1_MODEL_CONFIG_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = px.line(kuhn_perf_df.drop_duplicates(), x=\"total_episodes\", y=\"exploitability\", title=f\"Exploitability over Episodes Kuhn 3 workers\",\n",
    "        render_mode=\"svg\", color=\"experiment_name\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kuhn_perf_df.to_csv(f\"kuhn_results_{datetime_str()}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}